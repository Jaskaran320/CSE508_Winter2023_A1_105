{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES_PATH = os.path.join(os.getcwd(), os.pardir, os.pardir, 'Dataset')\n",
    "pattern = r'<TITLE>(.*?)</TITLE>.*?<TEXT>(.*?)</TEXT>'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(FILES_PATH):\n",
    "\n",
    "    filepath = os.path.join(FILES_PATH, filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    # Extracting the desired contents and concatenate them with a space\n",
    "    # new_content = ' '.join(re.findall(pattern, content, flags=re.DOTALL)[0])\n",
    "    title, text = re.findall(pattern, contents, flags=re.DOTALL)[0]\n",
    "    new_contents = f'{title.strip()} {text.strip()}'\n",
    "\n",
    "    # Saving the new contents in the same file\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(new_contents)\n",
    "\n",
    "    # saving in alternate folder to avoid overwriting, uncomment the above lines to overwrite (just for testing)\n",
    "    # with open(os.path.join('../../Dataset/CSE508_Winter2023_Dataset', 'changed_files', filename), 'w') as f:\n",
    "    #     f.write(new_contents)\n",
    "    \n",
    "    # Print the contents of 5 sample files before and after performing the operation\n",
    "    if int(filename[10:14]) in [10, 50, 100, 500, 1000]:\n",
    "        print('File:', filename)\n",
    "        print('Before:', contents)\n",
    "        print('After:', new_contents)\n",
    "        print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''alternate code for extracting title and text'''\n",
    "\n",
    "# # Iterating over all files in the folder\n",
    "# for filename in os.listdir(FILES_PATH):\n",
    "\n",
    "#     # if filename.startswith('cranfield'):\n",
    "#     filepath = os.path.join(FILES_PATH, filename)\n",
    "#     with open(filepath, 'r') as f:\n",
    "#         contents = f.read()\n",
    "    \n",
    "#     # Extracting the contents in the title tag\n",
    "#     title_start = contents.find('<TITLE>') + len('<TITLE>')\n",
    "#     title_end = contents.find('</TITLE>', title_start)\n",
    "#     title = contents[title_start:title_end].strip()\n",
    "    \n",
    "#     # Extracting the contents in the text tag\n",
    "#     text_start = contents.find('<TEXT>') + len('<TEXT>')\n",
    "#     text_end = contents.find('</TEXT>', text_start)\n",
    "#     text = contents[text_start:text_end].strip()\n",
    "    \n",
    "#     # Concatenating the contents\n",
    "#     new_contents = title + ' ' + text\n",
    "    \n",
    "#     # Save the new contents in the same file\n",
    "#     # with open(filepath, 'w') as f:\n",
    "#     #     f.write(new_contents)\n",
    "    \n",
    "#     # Printing the contents of 5 sample files before and after performing the operation\n",
    "#     if int(filename[10:14]) in [10, 50, 100, 500, 1000]:\n",
    "#         print('File:', filename)\n",
    "#         print('Before:', contents)\n",
    "#         print('After:', new_contents)\n",
    "#         print('----------------------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jaskaran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jaskaran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = '../../Dataset/CSE508_Winter2023_Dataset/changed_files/'\n",
    "\n",
    "for filename in os.listdir(FILES_PATH):\n",
    "\n",
    "    filepath = os.path.join(FILES_PATH, filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    if int(filename[10:14]) in [1, 10, 100, 500, 1000]:\n",
    "        print('File:', filename)\n",
    "        print('Before:', contents)\n",
    "\n",
    "    # Lowercasing the text\n",
    "    contents = contents.lower()\n",
    "    if int(filename[10:14]) in [10, 50, 100, 500, 1000]:\n",
    "        print('After lowercase:', contents)\n",
    "\n",
    "    # Performing tokenization\n",
    "    tokens = word_tokenize(contents)\n",
    "    if int(filename[10:14]) in [10, 50, 100, 500, 1000]:\n",
    "        print('After tokenization:', tokens)\n",
    "\n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    if int(filename[10:14]) in [10, 50, 100, 500, 1000]:\n",
    "        print('After removing stopwords:', tokens)\n",
    "\n",
    "    # Removing punctuations\n",
    "    tokens = [token for token in tokens if token not in punctuation]\n",
    "    if int(filename[10:14]) in [10, 50, 100, 500, 1000]:\n",
    "        print('After removing punctuations:', tokens)\n",
    "\n",
    "    # Removing blank space tokens\n",
    "    tokens = [token for token in tokens if token.strip()]\n",
    "    if int(filename[10:14]) in [10, 50, 100, 500, 1000]:\n",
    "        print('After removing blank space tokens:', tokens)\n",
    "    \n",
    "    new_contents = ' '.join(tokens)\n",
    "\n",
    "    # Save the new contents in the same file\n",
    "    # with open(filepath, 'w') as f:\n",
    "    #     f.write(new_contents)\n",
    "\n",
    "    if int(filename[10:14]) in [10, 50, 100, 500, 1000]:\n",
    "        print('----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b0a2ac5495fd4e93915ee863f3c8ec56995a9e8af8c986d1cc694d300643706"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
