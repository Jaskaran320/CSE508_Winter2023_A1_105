{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List, Dict, Tuple, Union\n",
    "import string\n",
    "import pickle\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "ORIGINAL_PATH = os.path.join(os.getcwd(), os.pardir, os.pardir, 'Dataset')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# word_list = ['Number', 'of', 'words', 'in', 'a', 'document', 'is', 'not', 'fixed', 'and', 'can', 'vary', 'from', 'document', 'to', 'document', 'but', 'the',\n",
    "# \t\t\t 'average', 'number', 'of', 'words', 'in', 'a', 'document', 'is', 'around', '200', 'to', '300', 'words', 'and', 'the', 'maximum', 'number', 'of', 'words', 'in', 'a', 'document', 'is', 'around', '1000', 'words']\n",
    "# path = os.path.join(os.getcwd(), 'DS2')\n",
    "# os.makedirs(\"DS2\")\n",
    "# for i2 in range(10):\n",
    "# \trandom.seed(i2)\n",
    "# \trandom_indexes = random.sample(range(0, len(word_list)), 10)\n",
    "# \tfilename1 = \"a\" + str(i2+1).zfill(2)\n",
    "#\n",
    "# \twith open(os.path.join(path, filename1), 'w') as f:\n",
    "# \t\tfor j2 in random_indexes:\n",
    "# \t\t\tf.write(word_list[j2] + \" \")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class BigramIndex:\n",
    "    def __init__(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the BigramIndex object.\n",
    "        :param path: The path to the collection of documents.\n",
    "        \"\"\"\n",
    "\n",
    "        self.docs: int = 0\n",
    "        self.index: Dict[str, Tuple[int, List[str]]] = {}\n",
    "        self.PATH: str = path\n",
    "        self.buildIndex()\n",
    "\n",
    "\n",
    "    def buildIndex(self) -> None:\n",
    "        \"\"\"\n",
    "        Build the bigram inverted index by processing each document in the collection.\n",
    "        Process includes lower-casing, tokenizing, removing stopwords, punctuations and blank spaces.\n",
    "        \"\"\"\n",
    "\n",
    "        for filename in os.listdir(self.PATH):\n",
    "            self.docs += 1\n",
    "\n",
    "            with open(os.path.join(self.PATH, filename), 'r') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            # Tokenize and lower case\n",
    "            tokens = word_tokenize(content.lower())\n",
    "            # Remove stopwords\n",
    "            tokens = [token for token in tokens if token not in stopwords.words(\"english\")]\n",
    "            # Remove punctuation\n",
    "            tokens = [token.translate(str.maketrans(\"\", \"\", string.punctuation)) for token in tokens]\n",
    "            # Remove blank spaces\n",
    "            tokens = [token for token in tokens if token.strip()]\n",
    "\n",
    "            # Add biwords to index\n",
    "            for b in range(len(tokens) - 1):\n",
    "                biword = tokens[b] + \" \" + tokens[b+1]\n",
    "                if biword not in self.index:\n",
    "                    self.index[biword] = (0, [filename])\n",
    "                else:\n",
    "                    self.index[biword][1].append(filename)\n",
    "\n",
    "            # Sort unique postings\n",
    "            for token in self.index:\n",
    "                self.index[token] = (len(self.index[token][1]), sorted(list(set(self.index[token][1]))))\n",
    "\n",
    "        print(\"Finished Building Index\")\n",
    "\n",
    "\n",
    "    def getPostingList(self, term: str) -> Tuple[int, List[str]]:\n",
    "        \"\"\"\n",
    "        Get the posting list for a term.\n",
    "        :param term: The term to get the posting list for.\n",
    "        :return: The posting list for the term as well as the frequency of the term in the collection.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.index[term] if term in self.index else (0, [])\n",
    "\n",
    "    def singleWordPostingList(self, term: str) -> Tuple[int, List[str]]:\n",
    "        \"\"\"\n",
    "        Get the posting list for a term.\n",
    "        :param term: The term to get the posting list for.\n",
    "        :return: The posting list for the term as well as the frequency of the term in the collection.\n",
    "        \"\"\"\n",
    "\n",
    "        posting_list = []\n",
    "        # Search all keys in index which contain the term and append the posting list to the list\n",
    "        for key in self.index:\n",
    "            if term in key:\n",
    "                posting_list.append(self.index[key][1])\n",
    "\n",
    "        # Sort unique postings\n",
    "        posting_list = sorted(list(set(posting_list)))\n",
    "\n",
    "        return len(posting_list), posting_list\n",
    "\n",
    "    def getTotalDocs(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the total number of documents in the collection.\n",
    "        :return: The total number of documents in the collection.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.docs\n",
    "\n",
    "\n",
    "    def queryAND(self, term1: Union[str, List[str]], term2: Union[str, List[str]]) -> Tuple[List[str], int]:\n",
    "        \"\"\"\n",
    "        Perform a boolean AND query on the bigram inverted index.\n",
    "        :param term1: The first term to query or a posting list.\n",
    "        :param term2: The second term to query or a posting list.\n",
    "        :return: A list of document names that contain both terms and the number of comparisons made.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if terms are in inverted index\n",
    "        if not isinstance(term1, list) and term1 not in self.index:\n",
    "            return [], 0\n",
    "        if not isinstance(term2, list) and term2 not in self.index:\n",
    "            return [], 0\n",
    "        if isinstance(term1, list) and len(term1) == 0:\n",
    "            return [], 0\n",
    "        if isinstance(term2, list) and len(term2) == 0:\n",
    "            return [], 0\n",
    "\n",
    "\n",
    "        postings1 = self.index[term1][1] if isinstance(term1, str) else term1\n",
    "        postings2 = self.index[term2][1] if isinstance(term2, str) else term2\n",
    "\n",
    "        # Perform AND\n",
    "        comparisons = 0\n",
    "        result = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while i < len(postings1) and j < len(postings2):\n",
    "            comparisons += 1\n",
    "            if postings1[i] == postings2[j]:\n",
    "                result.append(postings1[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif postings1[i] < postings2[j]:\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "\n",
    "        return result, comparisons\n",
    "\n",
    "\n",
    "    def queryProcess(self, query: str) -> Tuple[List[str], int]:\n",
    "        \"\"\"\n",
    "        Parse a query and perform the appropriate boolean query. Only AND query is supported.\n",
    "        :param query: The query to parse, should be of the form \"A B AND B C AND C D\" or a similar combination.\n",
    "        :return: List of document names that match the query and the number of comparisons made.\n",
    "        \"\"\"\n",
    "\n",
    "        result = []\n",
    "        comparisons = 0\n",
    "\n",
    "        # Split query into AND subqueries\n",
    "        and_subqueries = query.split(\" \")\n",
    "\n",
    "        if len(and_subqueries) == 1:\n",
    "            return self.singleWordPostingList(and_subqueries[0])[1], 0\n",
    "\n",
    "        bigram_subqueries = []\n",
    "        for s in range(len(and_subqueries) - 1):\n",
    "            bigram_subqueries.append(and_subqueries[s] + \" \" + and_subqueries[s+1])\n",
    "\n",
    "        # Process each AND subquery\n",
    "        for subquery in bigram_subqueries:\n",
    "            # If result is empty, get posting list for first term\n",
    "            if len(result) == 0:\n",
    "                result = self.index[subquery][1] if subquery in self.index else []\n",
    "                comparisons += 0\n",
    "            # Otherwise, perform AND query\n",
    "            else:\n",
    "                result, subcomparisons = self.queryAND(result, subquery)\n",
    "                comparisons += subcomparisons\n",
    "\n",
    "        return result, comparisons\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "class PositionalIndex:\n",
    "    def __init__(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the PositionalIndex object.\n",
    "        :param path: The path to the collection of documents.\n",
    "        \"\"\"\n",
    "\n",
    "        self.docs: int = 0\n",
    "        self.index: Dict[str, List[Union[int, Dict[str, List[str]]]]] = {}\n",
    "        self.PATH: str = path\n",
    "        self.buildIndex()\n",
    "\n",
    "\n",
    "    def buildIndex(self) -> None:\n",
    "        \"\"\"\n",
    "        Build the bigram inverted index by processing each document in the collection.\n",
    "        Process includes lower-casing, tokenizing, removing stopwords, punctuations and blank spaces.\n",
    "        Index stored as {token: [frequency, {doc1: [pos1, pos2, ...], doc2: [pos1, pos2, ...], ...}]}\n",
    "        \"\"\"\n",
    "\n",
    "        for filename in os.listdir(self.PATH):\n",
    "            self.docs += 1\n",
    "\n",
    "            with open(os.path.join(self.PATH, filename), 'r') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            # Tokenize and lower case\n",
    "            tokens = word_tokenize(content.lower())\n",
    "            # Remove punctuation\n",
    "            tokens = [token.translate(str.maketrans(\"\", \"\", string.punctuation)) for token in tokens]\n",
    "            # Remove blank spaces\n",
    "            tokens = [token for token in tokens if token.strip()]\n",
    "\n",
    "\n",
    "            # Add tokens to index\n",
    "            for t in range(len(tokens)):\n",
    "                if tokens[t] in stopwords.words(\"english\"):\n",
    "                    continue\n",
    "                if tokens[t] not in self.index:\n",
    "                    self.index[tokens[t]] = [1, {filename: [t+1]}]\n",
    "                else:\n",
    "                    if filename not in self.index[tokens[t]][1]:\n",
    "                        self.index[tokens[t]][0] += 1\n",
    "                        self.index[tokens[t]][1][filename] = [t+1]\n",
    "                    else:\n",
    "                        self.index[tokens[t]][0] += 1\n",
    "                        self.index[tokens[t]][1][filename].append(t+1)\n",
    "\n",
    "\n",
    "            # Sort unique postings\n",
    "            for token in self.index:\n",
    "                self.index[token][1] = {k: sorted(list(set(v))) for k, v in sorted(self.index[token][1].items())}\n",
    "\n",
    "\n",
    "    def getPostingList(self, term: str) -> Tuple[int, Dict[str, List[str]]]:\n",
    "        \"\"\"\n",
    "        Get the posting list for a term.\n",
    "        :param term: The term to get the posting list for.\n",
    "        :return: The posting list for the term as well as the frequency of the term in the collection.\n",
    "        \"\"\"\n",
    "\n",
    "        if term not in self.index:\n",
    "            return 0, {}\n",
    "\n",
    "        return self.index[term][0], self.index[term][1]\n",
    "\n",
    "\n",
    "    def positionalQuery(self, phrase: str) -> Tuple[Dict[str, List[str]], int]:\n",
    "        \"\"\"\n",
    "        Perform a positional query on the positional index.\n",
    "        :param phrase: The phrase to query.\n",
    "        :return: The number of documents that contain the phrase and a dictionary of the documents and their positions.\n",
    "        \"\"\"\n",
    "\n",
    "        # Phrase in form [\"term1\", \"term2\", ...]\n",
    "        # Check if all terms are in index and convert them to form [{\"term1\": (index, freq)}, {\"term2\": (index, freq)}, ...]\n",
    "        phrase = phrase.split(\" \")\n",
    "        terms = []\n",
    "\n",
    "        for p in range(len(phrase)):\n",
    "            if phrase[p] in stopwords.words(\"english\"):\n",
    "                continue\n",
    "            if phrase[p] not in self.index:\n",
    "                return {}, 0\n",
    "            else:\n",
    "                terms.append({phrase[p]: (p, self.index[phrase[p]][0])})\n",
    "\n",
    "        # Sort in ascending order of frequency\n",
    "        terms = sorted(terms, key=lambda x: list(x.values())[0][1])\n",
    "\n",
    "        # Get posting list for term with the lowest frequency and remove it from the list\n",
    "        result = self.index[list(terms[0].keys())[0]][1]\n",
    "        term_index = list(terms[0].values())[0][0]\n",
    "        terms.pop(0)\n",
    "\n",
    "        while len(terms) > 0:\n",
    "            # Get posting list and index in phrase for next term\n",
    "            posting_list = self.index[list(terms[0].keys())[0]][1]\n",
    "            index = list(terms[0].values())[0][0]\n",
    "            terms.pop(0)\n",
    "\n",
    "            # Perform positional query\n",
    "            for doc in list(result.keys()):\n",
    "                if doc not in posting_list:\n",
    "                    result.pop(doc)\n",
    "                else:\n",
    "                    # Get positions for term in phrase and term in posting list\n",
    "                    positions = result[doc]\n",
    "                    posting_positions = posting_list[doc]\n",
    "\n",
    "                    # Check if positions are within same distance of each other as in phrase\n",
    "                    matched_positions = []\n",
    "                    for p in range(len(positions)):\n",
    "                        for pp in range(len(posting_positions)):\n",
    "\n",
    "                            if posting_positions[pp] > positions[p] + abs(index - term_index):\n",
    "                                break\n",
    "\n",
    "                            if posting_positions[pp] - positions[p] == index - term_index:\n",
    "                                matched_positions.append(positions[p])\n",
    "\n",
    "                    # If no positions match, remove document from result otherwise update positions\n",
    "                    if len(matched_positions) == 0:\n",
    "                        result.pop(doc)\n",
    "                    else:\n",
    "                        result[doc] = matched_positions\n",
    "\n",
    "        # Update positions to be relative to phrase\n",
    "        for doc in result:\n",
    "            result[doc] = [p - term_index for p in result[doc]]\n",
    "\n",
    "        return result, len(result)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Building Index\n"
     ]
    }
   ],
   "source": [
    "# Create bigram and positional index\n",
    "bg = BigramIndex(ORIGINAL_PATH)\n",
    "psi = PositionalIndex(ORIGINAL_PATH)\n",
    "\n",
    "# Save indexes\n",
    "with open(\"BigramIndex.pickle\", \"wb\") as f:\n",
    "    pickle.dump(bg, f)\n",
    "\n",
    "with open(\"PositionalIndex.pickle\", \"wb\") as f:\n",
    "    pickle.dump(psi, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# Load indexes\n",
    "with open(\"BigramIndex.pickle\", \"rb\") as f:\n",
    "    bg = pickle.load(f)\n",
    "\n",
    "with open(\"PositionalIndex.pickle\", \"rb\") as f:\n",
    "    psi = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phrase Queries ---\n",
      "Enter the query: \n",
      "\n",
      "Number of documents retrieved for query 1 using bigram inverted index:  8\n",
      "Name of documents retrieved for query 1 using bigram inverted index: cranfield0007, cranfield0096, cranfield0207, cranfield0344, cranfield0418, cranfield0536, cranfield0610, cranfield1321\n",
      "Number of documents retrieved for query 1 using positional index:  8\n",
      "Name of documents retrieved for query 1 using positional index: cranfield0007, cranfield0096, cranfield0207, cranfield0344, cranfield0418, cranfield0536, cranfield0610, cranfield1321\n",
      "Number of documents retrieved for query 2 using bigram inverted index:  1\n",
      "Name of documents retrieved for query 2 using bigram inverted index: cranfield0007\n",
      "Number of documents retrieved for query 2 using positional index:  1\n",
      "Name of documents retrieved for query 2 using positional index: cranfield0007\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Phrase Queries ---\")\n",
    "print(\"Enter the query: \")\n",
    "print()\n",
    "\n",
    "n = int(input(\"Enter the number of queries: \"))\n",
    "\n",
    "\n",
    "for i1 in range(n):\n",
    "    search_term = input(\"Enter the search term: \")\n",
    "    # search_term = \"document in number fixed a a words\"\n",
    "\n",
    "    # tokenize, remove punctuation, remove blank strings for both indexes\n",
    "    search_term = word_tokenize(search_term.lower())\n",
    "    search_term = [s.translate(str.maketrans(\"\", \"\", string.punctuation)) for s in search_term]\n",
    "    search_term = [s for s in search_term if s.strip()]\n",
    "\n",
    "    # Remove stopwords for bigram index\n",
    "    search_term2 = [s for s in search_term if s not in stopwords.words(\"english\")]\n",
    "\n",
    "    query1 = \"\"\n",
    "    for j in range(len(search_term2)):\n",
    "        query1 += search_term2[j] + \" \"\n",
    "\n",
    "    query1 = query1.strip()\n",
    "\n",
    "    # print(\"Bigram Index\")\n",
    "    # print(\"Bigram Query\", i1 + 1, \": \", query)\n",
    "\n",
    "    result1, comparisons1 = bg.queryProcess(query1)\n",
    "    print(\"Number of documents retrieved for query\", i1+1, \"using bigram inverted index: \", len(result1))\n",
    "    print(\"Name of documents retrieved for query\", i1+1, \"using bigram inverted index: \", end=\"\")\n",
    "    for i2 in range(len(result1)):\n",
    "        print(result1[i2], end=\"\")\n",
    "        if i2 < len(result1) - 1:\n",
    "            print(\", \", end=\"\")\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "    query1 = \"\"\n",
    "    for j1 in range(len(search_term)):\n",
    "        query1 += search_term[j1] + \" \"\n",
    "\n",
    "    query1 = query1.strip()\n",
    "\n",
    "    # print(\"Positional Index\")\n",
    "    # print(\"Positional Query\", i1 + 1, \": \", query1)\n",
    "\n",
    "    result2, len_result2 = psi.positionalQuery(query1)\n",
    "    print(\"Number of documents retrieved for query\", i1+1, \"using positional index: \", len_result2)\n",
    "    print(\"Name of documents retrieved for query\", i1+1, \"using positional index: \", end=\"\")\n",
    "\n",
    "    if result2:\n",
    "        for doc1 in result2:\n",
    "            print(doc1, end=\"\")\n",
    "            if doc1 != list(result2.keys())[-1]:\n",
    "                print(\", \", end=\"\")\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
